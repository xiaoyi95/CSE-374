Name: Tiffany Luu, Xiaoyi Ling

Group identifier: bi

Work divide: Tiffany Luu was responsible for getmem.c, get_mem_stats.c and mem.h. Xiaoyi Ling was responsible for freemem.c, mem_utils.c and print_heap.c. We both made contributions to bench.c and mem_impl.h but Tiffany did most of parts for bench.c.

Description of heap data structure: Our heap is a series linked list called free_node. Each free_node has a field called "size" which stores the size of this block. Note that this size excludes the size of the header. We also have a field called called "next" that is a pointer pointing to the header of next block. We use a global free_list variable as the start of the entire free list. It has an assigned size of 0 and a pointer pointing to the first block in the list. The blocks are orgnized in an order of increasing address so that we know where to put them back when user wants to free them, and also to check whether blocks need to be recombined. We ask for more blocks from malloc when we don't have a block big enough in the free list.

Summary of additional features: We set the minimum size difference of when we should split the block instead of taking the entire as a value in getmem. Currently it is defaulted to be 64. It means when the size of the block requested by the user rounding to the multiple of 16 is larger than then size of the block in the free list when we are searching by 64, we will split the block the get part of it. When the block in the freelist is larger than the block requested but the difference is smaller by 64, we will get the entire block. As for the minimum size case, we decided to set the minimum size equal to the split threshold. When the block requested by the user is smaller than the minimum size 64, we give them a block of size 64. So we experimented with a request of size 16, and the getmem returned the block with a size of at least 64.

Summary of results: We experimented with the program in several ways. We initially made a controlled, small number of calls to getmem/freemem to check that it functioned well in all cases. We then implemented the bench program and changed the total calls of freeing memory and getting memory between 10000, 100000 and 1000000. The total CPU time used is basically zero for 10000 times and increase to below 0.1 s for 100000 and tens of seconds for 1000000. The number of free blocks in the list and the size of space we asked from malloc also increase. We then changed to percentage of larger blocks to be called to get memory to be 50, the CPU time and space asked from malloc obviously increased. The system became ineffecient when the calles increase to 1000000. The CPU time used to run the program varied from tens of seconds to several minutes in a extreme case. We also change the small limit and large limit size of block that request from the freelist. The CPU time and size of space asked from malloc also increase as the increase of these two value. Experimentingg with different default min_split values, we saw that increasing this value will reduce the number of small blocks at the beginning of the list, but will also return larger blocks to small calls. Decreasing this number will do the opposite.

Source: Wikipedia, Stackflow, source code of malloc
